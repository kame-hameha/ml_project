{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6bb1bd-f7ad-42b9-a5ba-1e1b72e36c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 21:54:24.991689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/kai/.local/lib/python3.12/site-packages/tensorflow_hub/__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Created By:     Kai Metzger\n",
    "# Created School: Franz-Oberthuer-Schule Wuerzburg\n",
    "# Created Email:  metzgerkai@franz-oberthuer-schule.de\n",
    "# Created Date:   Mon Dec 01 17:25 UTC 2025\n",
    "# Version:        1.0.0\n",
    "# =============================================================================\n",
    "\"\"\"The Module has been build for training the symbols dataset with images + \n",
    "ground truth. Images have the base dimenstions of 640px x 480px in datasets 1-8\n",
    "and then another 700 images with resolution 1920px x 1080px have been added to  \n",
    "the subsequent datasets.\n",
    "The files in the dataset <symbols> should be ordered in the following\n",
    "manner:             > explanation\n",
    "- symbols      \n",
    "  - dataset1        > dataset version (dataset1 - 3 where used in the project)\n",
    "    - data          > images (.png files)\n",
    "    - gt            > ground truth (.txt files with class labels 1-4)\n",
    "\n",
    "  - # dataset/      e.g. daset10 the corresponding classes are stored in folders 0 - 3:\n",
    "    #     classA/     = cross\n",
    "    #     classB/     = circle \n",
    "    #     classC/     = triangle\n",
    "    #     classD/     = rectangle\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# Import\n",
    "# =============================================================================\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras import layers, models, utils, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# Enable Nvidia GPUs by un-commenting this line \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "XLA_FLAGS=\"--xla_gpu_cuda_data_dir=/usr/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ff7b29-4af4-4a85-8c39-82fa4c64c92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: /home/kai/git/ml_project/datasets/symbols/dataset10\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Declare variables\n",
    "# =============================================================================\n",
    "# Change the following paths to your dataset path\n",
    "home_dir = os.path.expanduser(\"~/git\")\n",
    "dataset_for_training = home_dir + \"/ml_project/datasets/symbols/dataset10\"\n",
    "\n",
    "print(\"Dataset:\", dataset_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08445a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764622469.865969   10300 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4617 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Dataset related\n",
    "# =============================================================================\n",
    "NUM_CLASSES = 4\n",
    "IMG_SIZE_X = 160\n",
    "IMG_SIZE_Y = 160\n",
    "\n",
    "IMG_SIZE = (IMG_SIZE_X, IMG_SIZE_Y)\n",
    "#INPUT_SHAPE = (IMG_SIZE_X, IMG_SIZE_Y, 1) # grayscale\n",
    "INPUT_SHAPE = (IMG_SIZE_X, IMG_SIZE_Y, 3) # color\n",
    "\n",
    "# =============================================================================\n",
    "# Hyperparameters\n",
    "# =============================================================================\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 300\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Set up an exponential decay learning rate schedule\n",
    "LR_SCHEDULE = ExponentialDecay(\n",
    "    initial_learning_rate=LEARNING_RATE,  # start with 0.001\n",
    "    decay_steps=100000,           # how often to apply the decay\n",
    "    decay_rate=0.96,              # decay rate\n",
    "    staircase=True                # whether to apply the decay in steps\n",
    ")\n",
    "\n",
    "OPTIMIZER = keras.optimizers.Adam(learning_rate=LR_SCHEDULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0437a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1400 files belonging to 4 classes.\n",
      "Using 1120 files for training.\n",
      "Found 1400 files belonging to 4 classes.\n",
      "Using 280 files for validation.\n",
      "Klassen: ['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (160, 160)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_for_training,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",           # 4 Klassen → 1-hot\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_for_training,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(\"Klassen:\", class_names)\n",
    "\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b466b238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resize_and_rescale_grayscale = tf.keras.Sequential([\\n    layers.Lambda(lambda x: tf.image.rgb_to_grayscale(x)),  # Convert to grayscale\\n    layers.Resizing(IMG_SIZE_X, IMG_SIZE_Y),        # Resize image to the target size\\n    layers.Rescaling(1./255)                        # Rescale pixel values to [0, 1]\\n])\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "# Data Augmentation\n",
    "# =============================================================================\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "\"\"\"resize_and_rescale_grayscale = tf.keras.Sequential([\n",
    "    layers.Lambda(lambda x: tf.image.rgb_to_grayscale(x)),  # Convert to grayscale\n",
    "    layers.Resizing(IMG_SIZE_X, IMG_SIZE_Y),        # Resize image to the target size\n",
    "    layers.Rescaling(1./255)                        # Rescale pixel values to [0, 1]\n",
    "])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b2bbb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MobileNetV2 (Transfer Learning)\n",
    "# =============================================================================\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=IMG_SIZE + (3,),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=IMG_SIZE + (3,))\n",
    "\n",
    "x = data_augmentation(inputs)\n",
    "x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "\n",
    "x = base_model(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0539b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_160            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,124</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ true_divide (\u001b[38;5;33mTrueDivide\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ subtract (\u001b[38;5;33mSubtract\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_160            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │         \u001b[38;5;34m5,124\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,263,108</span> (8.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,263,108\u001b[0m (8.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,124</span> (20.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,124\u001b[0m (20.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fc8a6d1-3af3-4251-9636-0baa5c2a69a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 21:54:42.303982: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 159ms/step - accuracy: 0.2804 - loss: 1.6541 - val_accuracy: 0.3286 - val_loss: 1.4669\n",
      "Epoch 2/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 0.3518 - loss: 1.3893 - val_accuracy: 0.4250 - val_loss: 1.2681\n",
      "Epoch 3/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 127ms/step - accuracy: 0.4821 - loss: 1.1614 - val_accuracy: 0.5464 - val_loss: 1.1085\n",
      "Epoch 4/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - accuracy: 0.5357 - loss: 1.0594 - val_accuracy: 0.6321 - val_loss: 0.9884\n",
      "Epoch 5/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.5866 - loss: 0.9832 - val_accuracy: 0.6857 - val_loss: 0.8936\n",
      "Epoch 6/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.6348 - loss: 0.9133 - val_accuracy: 0.7214 - val_loss: 0.8163\n",
      "Epoch 7/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 114ms/step - accuracy: 0.7027 - loss: 0.8096 - val_accuracy: 0.7500 - val_loss: 0.7562\n",
      "Epoch 8/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 114ms/step - accuracy: 0.7196 - loss: 0.7512 - val_accuracy: 0.7750 - val_loss: 0.7069\n",
      "Epoch 9/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.7402 - loss: 0.7339 - val_accuracy: 0.7750 - val_loss: 0.6649\n",
      "Epoch 10/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - accuracy: 0.7437 - loss: 0.7057 - val_accuracy: 0.7964 - val_loss: 0.6306\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Training (Feature Extraction)\n",
    "# =============================================================================\n",
    "initial_epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=initial_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7331d7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 133ms/step - accuracy: 0.6357 - loss: 0.8999 - val_accuracy: 0.8321 - val_loss: 0.5290\n",
      "Epoch 11/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - accuracy: 0.7402 - loss: 0.7196 - val_accuracy: 0.8500 - val_loss: 0.4733\n",
      "Epoch 12/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 130ms/step - accuracy: 0.7625 - loss: 0.6270 - val_accuracy: 0.8571 - val_loss: 0.4346\n",
      "Epoch 13/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - accuracy: 0.8018 - loss: 0.5314 - val_accuracy: 0.8679 - val_loss: 0.4020\n",
      "Epoch 14/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 130ms/step - accuracy: 0.8196 - loss: 0.4929 - val_accuracy: 0.8714 - val_loss: 0.3725\n",
      "Epoch 15/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - accuracy: 0.8438 - loss: 0.4335 - val_accuracy: 0.8857 - val_loss: 0.3468\n",
      "Epoch 16/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step - accuracy: 0.8670 - loss: 0.3805 - val_accuracy: 0.8893 - val_loss: 0.3309\n",
      "Epoch 17/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 132ms/step - accuracy: 0.8741 - loss: 0.3487 - val_accuracy: 0.8929 - val_loss: 0.3155\n",
      "Epoch 18/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - accuracy: 0.8893 - loss: 0.3178 - val_accuracy: 0.8929 - val_loss: 0.3049\n",
      "Epoch 19/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - accuracy: 0.8982 - loss: 0.2880 - val_accuracy: 0.8964 - val_loss: 0.2953\n",
      "Epoch 20/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - accuracy: 0.9098 - loss: 0.2681 - val_accuracy: 0.8964 - val_loss: 0.2897\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Fine Tuning\n",
    "# =============================================================================\n",
    "base_model.trainable = True\n",
    "fine_tune_from = 100\n",
    "\n",
    "for layer in base_model.layers[:fine_tune_from]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-5),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "fine_tune_epochs = 10\n",
    "total_epochs = initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=history.epoch[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "227dff53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.8964 - loss: 0.2897\n",
      "Test Accuracy: 0.8964285850524902\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Test Accuracy (Validation = Test)\n",
    "# =============================================================================\n",
    "loss, acc = model.evaluate(val_ds)\n",
    "print(\"Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f535f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Klassen: ['0', '1', '2', '3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 21:56:30.964171: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86        62\n",
      "           1       0.96      0.98      0.97        84\n",
      "           2       0.88      0.80      0.84        65\n",
      "           3       0.91      0.87      0.89        69\n",
      "\n",
      "    accuracy                           0.90       280\n",
      "   macro avg       0.89      0.89      0.89       280\n",
      "weighted avg       0.90      0.90      0.90       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Konfusionsmatrix + Klassifizierungsreport\n",
    "# =============================================================================\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Daten aus Validation/Test holen\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in val_ds:\n",
    "    preds = model.predict(images)\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Klassenliste\n",
    "print(\"Klassen:\", class_names)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d57183ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelle gespeichert: model.keras/\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Modell speichern\n",
    "# =============================================================================\n",
    "keras.saving.save_model(model, \"model.keras\")                 # Keras/H5 Format\n",
    "\n",
    "print(\"Modelle gespeichert: model.keras/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c5a4642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelle erfolgreich geladen!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Modell laden\n",
    "# =============================================================================\n",
    "loaded_model = keras.models.load_model(\"model.keras\")\n",
    "\n",
    "print(\"Modelle erfolgreich geladen!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eb07b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 173ms/step - accuracy: 0.8946 - loss: 0.2807 - val_accuracy: 0.8964 - val_loss: 0.2832\n",
      "Epoch 2/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - accuracy: 0.9045 - loss: 0.2552 - val_accuracy: 0.9036 - val_loss: 0.2748\n",
      "Epoch 3/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - accuracy: 0.9089 - loss: 0.2564 - val_accuracy: 0.9000 - val_loss: 0.2701\n",
      "Epoch 4/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 153ms/step - accuracy: 0.9107 - loss: 0.2413 - val_accuracy: 0.8893 - val_loss: 0.2648\n",
      "Epoch 5/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - accuracy: 0.9250 - loss: 0.2101 - val_accuracy: 0.9071 - val_loss: 0.2541\n",
      "Epoch 6/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9134 - loss: 0.2126 - val_accuracy: 0.9143 - val_loss: 0.2505\n",
      "Epoch 7/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - accuracy: 0.9384 - loss: 0.1908 - val_accuracy: 0.9107 - val_loss: 0.2488\n",
      "Epoch 8/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - accuracy: 0.9277 - loss: 0.1977 - val_accuracy: 0.9071 - val_loss: 0.2463\n",
      "Epoch 9/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - accuracy: 0.9464 - loss: 0.1674 - val_accuracy: 0.9071 - val_loss: 0.2379\n",
      "Epoch 10/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - accuracy: 0.9438 - loss: 0.1539 - val_accuracy: 0.9071 - val_loss: 0.2384\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# =============================================================================\n",
    "# TensorBoard Logging\n",
    "# =============================================================================\n",
    "log_dir = \"logs/fit\"\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "# Beim Training einfügen:\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=initial_epochs,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Starten in Shell:\n",
    "# tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346db74",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m124356608472032\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         ...,\\n         [100.5   , 117.    ,  76.5   ],\\n         [109.25  , 122.25  , 104.3125],\\n         [107.8125, 119.6875, 109.0625]],\\n\\n        [[  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         ...,\\n         [106.625 , 121.625 ,  90.875 ],\\n         [112.1875, 126.8125,  98.9375],\\n         [111.4375, 126.4375,  94.8125]],\\n\\n        [[  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         ...,\\n         [111.4375, 122.375 ,  95.0625],\\n         [109.8125, 128.    , 105.125 ],\\n         [115.4375, 129.4375, 104.4375]],\\n\\n        ...,\\n\\n        [[  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         ...,\\n         [ 14.25  ,  26.25  ,   0.    ],\\n         [ 20.    ,  32.    ,   0.    ],\\n         [ 29.875 ,  36.25  ,   0.    ]],\\n\\n        [[  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         ...,\\n         [ 20.5   ,  32.5   ,   0.    ],\\n         [ 17.1875,  29.1875,   0.    ],\\n         [ 20.5   ,  30.625 ,   0.    ]],\\n\\n        [[  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         ...,\\n         [ 16.25  ,  27.625 ,   0.    ],\\n         [ 18.75  ,  30.125 ,   0.    ],\\n         [ 14.875 ,  32.0625,   0.    ]]]],\\n      shape=(1, 160, 160, 3), dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m example_np = np.expand_dims(example_img, axis=\u001b[32m0\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Grad-CAM erzeugen\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m heatmap, predicted_class = \u001b[43mmake_gradcam_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_conv_layer_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Heatmap auf Original legen\u001b[39;00m\n\u001b[32m     43\u001b[39m img = example_img.numpy().astype(\u001b[33m\"\u001b[39m\u001b[33muint8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mmake_gradcam_heatmap\u001b[39m\u001b[34m(img_array, model, last_conv_layer_name)\u001b[39m\n\u001b[32m     11\u001b[39m grad_model = tf.keras.models.Model(\n\u001b[32m     12\u001b[39m     [model.inputs],\n\u001b[32m     13\u001b[39m     [model.get_layer(last_conv_layer_name).output, model.output]\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.GradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     conv_outputs, predictions = \u001b[43mgrad_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     pred_index = tf.argmax(predictions[\u001b[32m0\u001b[39m])\n\u001b[32m     19\u001b[39m     loss = predictions[:, pred_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/keras/src/ops/function.py:214\u001b[39m, in \u001b[36mFunction._run_through_graph\u001b[39m\u001b[34m(self, inputs, operation_fn, call_fn)\u001b[39m\n\u001b[32m    212\u001b[39m output_tensors = []\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.outputs:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     output_tensors.append(\u001b[43mtensor_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree.pack_sequence_as(\u001b[38;5;28mself\u001b[39m._outputs_struct, output_tensors)\n",
      "\u001b[31mKeyError\u001b[39m: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m124356608472032\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         ...,\\n         [100.5   , 117.    ,  76.5   ],\\n         [109.25  , 122.25  , 104.3125],\\n         [107.8125, 119.6875, 109.0625]],\\n\\n        [[  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         ...,\\n         [106.625 , 121.625 ,  90.875 ],\\n         [112.1875, 126.8125,  98.9375],\\n         [111.4375, 126.4375,  94.8125]],\\n\\n        [[  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         ...,\\n         [111.4375, 122.375 ,  95.0625],\\n         [109.8125, 128.    , 105.125 ],\\n         [115.4375, 129.4375, 104.4375]],\\n\\n        ...,\\n\\n        [[  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         ...,\\n         [ 14.25  ,  26.25  ,   0.    ],\\n         [ 20.    ,  32.    ,   0.    ],\\n         [ 29.875 ,  36.25  ,   0.    ]],\\n\\n        [[  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         ...,\\n         [ 20.5   ,  32.5   ,   0.    ],\\n         [ 17.1875,  29.1875,   0.    ],\\n         [ 20.5   ,  30.625 ,   0.    ]],\\n\\n        [[  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         [  5.    ,   5.    ,   5.    ],\\n         ...,\\n         [ 16.25  ,  27.625 ,   0.    ],\\n         [ 18.75  ,  30.125 ,   0.    ],\\n         [ 14.875 ,  32.0625,   0.    ]]]],\\n      shape=(1, 160, 160, 3), dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\""
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Grad-CAM Visualisierung\n",
    "# =============================================================================\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
    "    # Modell: Feature-Extraktor + Klassifikationslayer trennen\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs],\n",
    "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        pred_index = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(loss, conv_outputs)[0]\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n",
    "\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs[0]), axis=-1)\n",
    "\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap) + 1e-8\n",
    "    return heatmap.numpy(), int(pred_index.numpy())\n",
    "\n",
    "# Name der letzten Convolution-Schicht in MobileNetV2:\n",
    "last_conv_layer_name = \"Conv_1\"\n",
    "\n",
    "# Beispielbild (nimm erstes Bild aus val_ds)\n",
    "example_img, example_label = next(iter(val_ds))\n",
    "example_img = example_img[0]\n",
    "example_np = np.expand_dims(example_img, axis=0)\n",
    "\n",
    "# Grad-CAM erzeugen\n",
    "heatmap, predicted_class = make_gradcam_heatmap(example_np, model, last_conv_layer_name)\n",
    "\n",
    "# Heatmap auf Original legen\n",
    "img = example_img.numpy().astype(\"uint8\")\n",
    "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "superimposed_img = cv2.addWeighted(img, 0.6, heatmap_color, 0.4, 0)\n",
    "\n",
    "# Anzeigen\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Grad-CAM Heatmap\")\n",
    "plt.imshow(heatmap, cmap=\"jet\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(f\"Overlay (Predicted: {class_names[predicted_class]})\")\n",
    "plt.imshow(superimposed_img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6068b1d-0e2d-4161-a705-1901996eb838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(8):\n",
    "        ax = plt.subplot(1, 8, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757612c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69312162",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009c10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, _ in train_dataset.take(1):\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  first_image = image[0]\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "    plt.imshow(augmented_image[0] / 255)\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "rescale = tf.keras.layers.Rescaling(1./127.5, offset=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c735e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "#print(IMG_SIZE)\n",
    "#print(INPUT_SHAPE)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "#print(IMG_SHAPE)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=INPUT_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40981e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "image_batch, label_batch = next(iter(train_ds))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eedff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0217a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db670ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e587c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34dc959",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aefb9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(validation_dataset)\n",
    "\n",
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f09a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caef180",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=len(history.epoch),\n",
    "                         validation_data=validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a65c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e84081",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be422fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e672e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d383489a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
