{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5a6bb1bd-f7ad-42b9-a5ba-1e1b72e36c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Created By:     Kai Metzger\n",
    "# Created School: Franz-Oberthuer-Schule Wuerzburg\n",
    "# Created Email:  metzgerkai@franz-oberthuer-schule.de\n",
    "# Created Date:   Sat April 05 09:54 UTC 2025\n",
    "# Version:        1.0.1\n",
    "# =============================================================================\n",
    "\"\"\"The Module has been build for training the symbols dataset with images + \n",
    "ground truth on a Raspberry Pi 5 with a standard USB camera. An image with a \n",
    "resolution of 640px x 480px can be recorded with the Python script \n",
    "create-symbols-dataset.py.\n",
    "The files in the dataset <symbols> should be ordered in the following\n",
    "manner:             > explanation\n",
    "- symbols      \n",
    "  - dataset1        > dataset version (dataset1 - 3 where used in the project)\n",
    "    - data          > images (.png files)\n",
    "    - gt            > ground truth (.txt files with class labels 1-4)\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# Import\n",
    "# =============================================================================\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras import layers, models, utils, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable Nvidia GPUs by un-commenting this line \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "XLA_FLAGS=\"--xla_gpu_cuda_data_dir=/usr/\"\n",
    "\n",
    "# To get reproducable results with the same training setting random seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "keras.utils.set_random_seed = SEED\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "06ff7b29-4af4-4a85-8c39-82fa4c64c92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "/home/pi/ml_project/datasets/symbols/dataset1/data\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Declare variables\n",
    "# =============================================================================\n",
    "img_size_x = 32\n",
    "img_size_y = 32\n",
    "img_dim = img_size_x * img_size_y\n",
    "\n",
    "# Change the following paths to your dataset path\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "dataset_for_training = \"dataset1\"\n",
    "img_dir = home_dir + \"/ml_project/datasets/symbols/\" + dataset_for_training + \"/data\"\n",
    "gt_dir = home_dir + \"/ml_project/datasets/symbols/\" + dataset_for_training + \"/gt\"\n",
    "checkpoint_filepath = home_dir + \"/ml_project/datasets/symbols/\" + dataset_for_training + \"/chpt/\"\n",
    "\n",
    "print(img_dim)\n",
    "print(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b7f55d23-ec55-4744-b3f3-f6446628f422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 176\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Get number of data (image/label) \n",
    "# =============================================================================\n",
    "anz_data = len(os.listdir(img_dir))\n",
    "anz_data = int(anz_data)\n",
    "dataset = np.zeros((anz_data, img_size_x, img_size_y), dtype=float)\n",
    "ground_truth = np.zeros((anz_data), dtype=int)\n",
    "print(\"dataset size:\", anz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b7ee187c-6626-4702-96b7-9b63df810c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Define functions\n",
    "# =============================================================================\n",
    "# Change image size and convert to grayscale images\n",
    "def pic_prep (image, x, y):\n",
    "  image = cv2.resize(image, (y,x))                # change image size\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
    "  image = image / 255                             # image normalization\n",
    "  return image\n",
    "\n",
    "# Shuffle images\n",
    "def unison_shuffle(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "82ce2dab-c78d-482e-b431-4c4fee6925fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Callback for training\n",
    "# =============================================================================\n",
    "# Set verbose flag to 0 to omit loss/accuracy output per epoch\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath + \"chpt.keras\",\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='auto',\n",
    "    save_best_only=True,\n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2fc8a6d1-3af3-4251-9636-0baa5c2a69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Read dataset\n",
    "# =============================================================================\n",
    "for i in range(0, anz_data, 1):\n",
    "    img_name = img_dir + '/' + str(i) + '.png'      # create file names\n",
    "    image = cv2.imread(img_name)                    # read image\n",
    "    image = pic_prep(image, img_size_x, img_size_y) # prepare image\n",
    "    dataset[i,:,:] = image                          # 2d-image to 3d-array\n",
    "    \n",
    "    txt_name = gt_dir + '/' + str(i) + '.txt'       # create gt file names\n",
    "    ground_truth[i] = np.genfromtxt(txt_name, delimiter=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b6068b1d-0e2d-4161-a705-1901996eb838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176,)\n",
      "(176, 32, 32)\n",
      "(140,)\n",
      "(140, 32, 32)\n",
      "(140,)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Prepare dataset: train set (80%) and test set (20%)\n",
    "# =============================================================================\n",
    "#dataset = dataset.reshape(anz_data, img_dim) # convert into 2d array (all pixel in one row)\n",
    "#ground_truth = ground_truth.reshape(anz_data, 1)\n",
    "\n",
    "dataset, ground_truth = unison_shuffle(dataset, ground_truth)\n",
    "\n",
    "trainset = np.random.choice(dataset.shape[0],\n",
    "                            int(dataset.shape[0]*0.80), \n",
    "                            replace=False)\n",
    "train_data = dataset[trainset,:]\n",
    "train_gt = ground_truth[trainset]\n",
    "#train_gt = utils.to_categorical(train_gt, 4)\n",
    "\n",
    "testset = np.delete(np.arange(0, len(ground_truth) ), \n",
    "                    trainset) \n",
    "test_data = dataset[testset,:]\n",
    "test_gt = ground_truth[testset]\n",
    "#test_gt = utils.to_categorical(test_gt, 4)\n",
    "\n",
    "print(ground_truth.shape)\n",
    "print(dataset.shape)\n",
    "print(trainset.shape)\n",
    "print(train_data.shape)\n",
    "print(train_gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9a841dcc-75e7-487f-bd1e-856f2f57e173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" model = models.Sequential()\\nmodel.add(layers.Dense(64,input_dim=img_dim,activation='relu'))\\nmodel.add(layers.Dense(32,input_dim=img_dim,activation='relu'))\\nmodel.add(layers.Dense(32,input_dim=img_dim,activation='relu'))\\nmodel.add(layers.Dense(128,input_dim=img_dim,activation='relu'))\\nmodel.add(layers.Dense(4,activation='sigmoid'))\\nopt = keras.optimizers.Adam(learning_rate=0.0001)\\nmodel.compile(loss='mean_squared_error', \\n              optimizer=opt, \\n              metrics=['accuracy']) \""
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Create neural network with FC layers and some neurons per layer.\n",
    "# =============================================================================\n",
    "\"\"\" model = models.Sequential()\n",
    "model.add(layers.Dense(64,input_dim=img_dim,activation='relu'))\n",
    "model.add(layers.Dense(32,input_dim=img_dim,activation='relu'))\n",
    "model.add(layers.Dense(32,input_dim=img_dim,activation='relu'))\n",
    "model.add(layers.Dense(128,input_dim=img_dim,activation='relu'))\n",
    "model.add(layers.Dense(4,activation='sigmoid'))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(loss='mean_squared_error', \n",
    "              optimizer=opt, \n",
    "              metrics=['accuracy']) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138451b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_18\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_18\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_34 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_32 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_35 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_33 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_16 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">272,356</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m272,356\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">272,356</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m272,356\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Create neural network with 4 layers and (64, 32, 16, 4) neurons per layer.\n",
    "# =============================================================================\n",
    "model = models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[32, 32, 1]))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(layers.Dense(128,input_dim=img_dim,activation='relu'))\n",
    "model.add(layers.Dense(4,activation='softmax'))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='mean_squared_error', \n",
    "              optimizer=opt, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fc1e31e8-f09d-4de2-a1f3-d61cd34ad9b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(4,), output.shape=(4, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[207]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Train the neuronal network e.g. for 300 epochs.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtrain_gt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_gt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml_project/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml_project/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/nn.py:767\u001b[39m, in \u001b[36mbinary_crossentropy\u001b[39m\u001b[34m(target, output, from_logits)\u001b[39m\n\u001b[32m    764\u001b[39m output = tf.convert_to_tensor(output)\n\u001b[32m    766\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target.shape) != \u001b[38;5;28mlen\u001b[39m(output.shape):\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    768\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArguments `target` and `output` must have the same rank \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    769\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(ndim). Received: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    770\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    771\u001b[39m     )\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target.shape, output.shape):\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 != e2:\n",
      "\u001b[31mValueError\u001b[39m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(4,), output.shape=(4, 4)"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Train the neuronal network e.g. for 300 epochs.\n",
    "# =============================================================================\n",
    "history = model.fit(train_data, \n",
    "                    train_gt, \n",
    "                    batch_size=4, \n",
    "                    epochs=200, \n",
    "                    verbose=0, \n",
    "                    shuffle=True, \n",
    "                    validation_data=(test_data, test_gt), \n",
    "                    callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f04375-7a41-42df-8433-5eb7ae2f6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Plot train and val accuracy.\n",
    "# =============================================================================\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19880dd-f5d1-4a7e-978d-c641adc03a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Plot train and val loss.\n",
    "# =============================================================================\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2846b7-cfb6-4513-9e77-19860d3296cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load pretrained dataset weights to e.g. test on new (unseen) data.\n",
    "# =============================================================================\n",
    "model.load_weights(checkpoint_filepath + \"chpt.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7e194-064b-4c7d-915d-91034e42ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Test dataset on xxx.\n",
    "# =============================================================================\n",
    "score = model.evaluate(test_data, test_gt, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a90aef-2bd8-4b49-91c2-b98bed33405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Testing on a single image.\n",
    "# =============================================================================\n",
    "data_pred = np.zeros((1, img_size_x, img_size_y), dtype=float)\n",
    "img_pred = cv2.imread(home_dir + \"/ml_project/datasets/symbols/\" + dataset_for_training + \"/data/14.png\")\n",
    "img_pred = pic_prep(img_pred, img_size_x, img_size_y)\n",
    "data_pred[0,:,:] = img_pred\n",
    "data_pred = data_pred.reshape(1,img_dim)\n",
    "result = model.predict(data_pred)\n",
    "result = np.round(result, decimals=2)\n",
    "\n",
    "print(\"Probability for classes: (cross, circle, triangle, square) in percent\", \n",
    "      result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93faeb92-98ef-4786-af62-d1a1898aa3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Output class: \n",
    "# translate class label (0,1,2,3) to class (cross, circle, triangle, square).\n",
    "# =============================================================================\n",
    "\n",
    "max_res = 0\n",
    "res_index = 4\n",
    "for i in range(0, 4, 1):\n",
    "    if result[0,i] > max_res:\n",
    "        max_res = result[0,i]\n",
    "        res_index = i\n",
    "\n",
    "if res_index == 0:\n",
    "    print('Cross detected!')\n",
    "elif res_index == 1:\n",
    "    print('Circle detected!')\n",
    "elif res_index == 2:\n",
    "    print('Triangle detected!')\n",
    "elif res_index == 3:\n",
    "    print('Square detected!')\n",
    "elif res_index == 4:\n",
    "    print('Error!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029a8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project_kernel",
   "language": "python",
   "name": "ml_project_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
